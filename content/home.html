<main>
  <h1>tinygrad documentation</h1>
  <p>
    Welcome to the docs for tinygrad. This page is for users of the tinygrad
    library. tinygrad is not 1.0 yet, but it will be soon. The API has been
    pretty stable for a while.
  </p>
  <p>
    While you can <code>pip install tinygrad</code>, we encourage you to install
    from source:
  </p>
  <div class="code-block">
    git clone https://github.com/tinygrad/tinygrad.git<br />
    cd tinygrad<br />
    python3 -m pip install -e .
  </div>
  <p>
    After you have installed tinygrad, try the <a href="#">MNIST tutorial</a>.
  </p>
  <p>
    If you are new to tensor libraries, learn how to use them by solving puzzles
    from <a href="#">tinygrad-tensor-puzzles</a>.
  </p>
  <p>
    We also have <a href="#">developer docs</a>, and Di Zhu has created a
    <a href="#">bunch of tutorials</a> to help understand how tinygrad works.
  </p>

  <h2>tinygrad Usage</h2>
  <p>
    The main class you will interact with is <strong>Tensor</strong>. It
    functions very similarly to PyTorch, but has a bit more of a functional
    style. tinygrad supports many datatypes. All operations in tinygrad are
    lazy, meaning they won't do anything until you realize.
  </p>
  <p>
    tinygrad has a built in neural network library with some classes,
    optimizers, and load/save state management.
  </p>
  <p>
    tinygrad has a JIT to make things fast. Decorate your pure function with
    <code>TinyJit</code>
  </p>
  <p>
    tinygrad has amazing support for multiple GPUs, allowing you to shard your
    Tensors with <code>Tensor.shard</code>.
  </p>
  <p>
    To understand what training looks like in tinygrad, you should read
    <code>beautiful_mnist.py</code>.
  </p>
  <p>
    We have a <a href="#">quickstart guide</a> and a <a href="#">showcase</a>
  </p>
</main>
